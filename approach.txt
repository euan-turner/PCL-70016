The task baseline model achieved:
0.48 F1 on the official dev-set
0.49 F1 on the official test-set

Approach:
1. Fine-tune DeBERTa-v3-large (304M) using LoRA
Baseline evaluation -> F1=0.1737  P=0.0951  R=1.0000 (just fine-tuning classification head)

2. Sweep LoRA rank over (8, 16, 32, 64)
R=32 best -> F1=0.4441, P=0.4643, R=0.4257

3. Use keyword-stratified cross-validation
4. Oversample from PCL examples to improve balance

R=32 Final dev â†’  F1=0.5205  P=0.4770  R=0.5729

6. Threshold tuning in compute_metrics
7. Label smoothing in cross entropy
8. Reduce oversampling to avoid changing actual distribution too much
9. Add linear classifier on [CLS] to predict PCL category on PCL samples


6. Use span-level PCL annotations to swap spans and create new positive examples

7. Use back-translation to augment PCL examples and improve diversity

8. Maybe train a PCL category predictor attached to the same encoder as the binary classifier, so the encoder gets more signal



General pre-processing:
1. strip HT<L tags and decode HTML entities
2. Remove empty/near-empty samples (< 3 tokens)
3. Normalise whitespace and quotations
4. Maybe replace entitities with type tokens
5. Keyword-stratified train/val split
