The task baseline model achieved:
0.48 F1 on the official dev-set
0.49 F1 on the official test-set

Approach:
1. Fine-tune DeBERTa-v3-large (304M) using LoRA
Baseline evaluation -> F1=0.1737  P=0.0951  R=1.0000 (just fine-tuning classification head)

2. Sweep LoRA rank over (8, 16, 32, 64)
R=32 best -> F1=0.4441, P=0.4643, R=0.4257

3. Use keyword-stratified cross-validation
4. Oversample from PCL examples to improve balance
(very slow training, replace with class-weighted loss in 7)
R=32 -> F1=0.4272, P=0.3277, R=0.6310

5. Replace oversampling with class-weighted loss


6. Use span-level PCL annotations to swap spans and create new positive examples

7. Use back-translation to augment PCL examples and improve diversity

8. Maybe train a PCL category predictor attached to the same encoder as the binary classifier, so the encoder gets more signal



General pre-processing:
1. strip HT<L tags and decode HTML entities
2. Remove empty/near-empty samples (< 3 tokens)
3. Normalise whitespace and quotations
4. Maybe replace entitities with type tokens
5. Keyword-stratified train/val split
